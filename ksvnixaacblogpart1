Nixâ€Š-â€Š(AaC) Anything as Code:
Â Shells Packages Containers Systems & Moreâ€Š - â€ŠAll Reproducible Declarative &Â Reliable

1) ModernÂ Chaos:

Throughout history, we've solved countless challenges through evolving paradigms, innovative tools, and strategies. Now we're in the AI era ğŸ¤–, yet we still struggle with a decades-old problem:

I used to keep a README ğŸ“„ full of setup steps.
Then I had a Makefile ğŸ› ï¸. Then a Dockerfile ğŸ³.
Then a setup.sh, a requirements.txt ğŸ“¦, and a ci.yml ğŸ¤–.
And stillâ€¦ things broke. ğŸ’£
Someone would clone the repo and ask:
"What version of Python does this need? ğŸ"
"Do I install that with Brew ğŸº or Pipenv ğŸ“¦?"
"Why doesn't the Docker build work on my machine? ğŸ’»"

In today's fragmented tooling landscape ğŸŒ, projects rely on multiple moving parts:
Language-specific package managers
Container runtimes
System setup scripts
CI configurations, etc

Even in the age of AI, we still face these same issues. Why? The reason for all these are the follows:

1.1) Divergent System (Known state â¡ï¸ Unknown state)Â ğŸ§©

âŒ Everything is Imperative ğŸ“œ

We instruct how to get things done, instead of declaring what we want. Like asking someone to make pasta ğŸâ€Š-â€Ševeryone does it differently even if ingredients are the same. Very careful recipe might work for pasta, but not for the software deployments where minute difference leads to inconsistencies and failures.
We've become accustomed to this behavior through repeated troubleshooting and temporary fixes. But then, as the modern and future practices evolve and scale, this approach makes the hell seep through these small cracks and make portable VMs, container or any type of deployment a painful process requiring frequent or occasional intervention costing budget, productivity and the overall reliability of the deployment pipelineğŸ˜“.

âœ… Solution: Declarative ApproachÂ ğŸ§¾

Moving Towards Declarative Systems

A significant improvement came with the adoption of config files âš™ï¸ over traditional scripts ğŸ“œ. This shift elegantly solved the imperative challenge by introducing a Declarative âœ¨ paradigm.

Why Declarative Works?

Instead of instructing how to achieve a result, you precisely define what you want ğŸ¯.

Advantages of the Declarative Paradigm:
Portable ğŸŒ: Configurations can be easily shared and applied across environments.
Atomic ğŸ’¥: Operations are allâœ…-or-nothingâŒ, preventing partial application and inconsistent states. This eliminated the ğŸ¤• headache of manually reverting changes after mid-script failures.
Reproducibility ğŸ”„ (to a degree): Applying the same configuration should yield similar outcomes.
Tracked Changes ğŸ‘€: Differences from previous configurations are easily discernible at a glance.

This approach solves many app-level issues. But system-wide problems remain. The diverse toolchain, language specific package managers all doesn't follow the declarative paradigm. Even if they all follow, the problem isn't solved there. Because the environment on which they all work and depend on isn't the same. Which brings us to the next problem...

1.2) Dependency Hell ğŸ˜ˆ
â—Reason 1) TLDRÂ : It's the culprit Imperative again
Even identical systems diverge over time as we install/update software/OS, run scripts etc etc. So 100 different system even if started and maintained the same way it won't be the same after some point. So over time we either lose track of changes or even if we track the change, the programs it depend on might vary and the scripts won't always work the same. Adjusting scripts for newer OS versions becomes routine ğŸ˜®â€ğŸ’¨. That's just temporarily filling the cracks, not a perfect solution and also a headache to maintain and rollback if the script fails midway because of the nature of script(being imperative).

â—Reason 2) Existing nature of the conventional Package managers and repo models ğŸ“¦ğŸŒ.
In most operating systems and languages, dependencies are managed globally. That means when you install a package, it often gets placed in a shared system pathâ€Š-â€Šlike /usr/lib. And traditional package managers assume there's one "correct" state/version for the whole system. 

But in reality, every modern project might need a slightly different set of tools. Even if same tool, the versions might vary and even if same version, the flags might vary. Modern programs became super customized specific to their needs, so the traditional globally managed package managers doesn't allow 2 different versions of python(python 3.7 and 3.10.) to exist peacefully, needless to say about about customized flags of same versions. Even if you manage it, their own dependencies might conflict (e.g., libc versions ğŸ§¨).

Language-specific solutions like virtualenv help. But still the python versions coexistence problem exists and of course multiple languages having multiple such package managers only makes things complex to adopt the ecosystem and also making cross-language dependency management a nightmare ğŸ§ ğŸ’¼. This brings the popular problem, "It works on my machine", but not working elsewhere.

ContainersğŸ³:Â 

The actual problem lies in the package managementğŸ“¦ level, but people did workarounds with VMs since it's a ready solution accepting the overheadğŸ˜•. And people started to solve things from the VMs POV and made containersğŸ³. Container itself is a neat alternative for VMs giving more benefits than a VM. Yes, great. Now people who thought VM(virtualising hardware) as overhead started accepting the Container(virtualising kernel) as a better solution for scalabilityğŸ˜. Containers are used to solve the VMs but still an overhead for packaging and development environmentsğŸ˜. Especially to keep the environments in sync u have to manually interveneğŸ˜®â€ğŸ’¨.

Don't get me wrong, containers are a great way for deploymentsğŸ‘ considering that's the only proper way for isolation other than VMs now. And the whole world is using docker and made their DevOps or any workflow that way. People went from VM route to Docker for solving scaling issues. But did not solve from the package management routeğŸ¤§. This seems to solve the dependency hell and scalability, but still did not solve the imperative small cracks I mentioned at the start and bring another problem "Lack of Reproducibility". Especially the Dockerfile.


Why DockerfileğŸ“œ is not good? (Guess the culprit againğŸ˜¶â€ğŸŒ«ï¸)

Let's have a very basic dockerfile example:

FROM ubuntu:latest
RUN apt-get update && \
 apt-get install -y curl wget && rm -rf /var/lib/apt/lists/*
WORKDIR /app
COPY . .
CMD ["curl" "--version"]

Dockerfile just again is another imperative thingğŸ˜®â€ğŸ’¨ which we just tell some instructions to arrive at a solution(Imperative). Now if u make 100 docker images from this file, all have the same environment right? ğŸ¤§NopeğŸ˜³. If u create an OCI image with dockerfile today and after some months or years the 2 images vary because, again we use traditional package managers inside it , whose repo maintainers might :
drop certain version because of end of LTS â¡ï¸ğŸ—‘ï¸ğŸ“… or
maybe they might have deleted wget or curl from their repo â¡ï¸âŒğŸ“¦ and 
needless to say the change in their versions â¡ï¸ğŸ”„ğŸ”¢. 

One clever way is to make a package for every version commit in the source code âœ¨ğŸ“¦. But all these traditional package managers ship the binaries they have already built. So it's not storage efficientğŸ’¾âŒ to keep GBs or TBs of data for different version of a single package curl. And pinning the version in these commands is tediousğŸ˜®â€ğŸ’¨ and of course won't work the same after months or years if something in the repo changed. And all the problems we mentioned earlier still lives inside this containerğŸ˜µ. 

Same Old Story, Different Surface:
Earlier: You made imperative steps(scripts) to set things up in VMs.ğŸ™‚
Now: You make imperative steps(dockerfile) to set things up in Docker.ğŸ« 
Both creating the imperative cracksğŸ’”.
 
TLDR: Docker is okayishğŸ˜¶, but dockerfileğŸ’€ isn't at all.

How do we actually solve ğŸ¤”: 
Imperative cracks? ğŸ’¥ğŸ§±
Diverging environments? â†”ï¸ğŸŒ
Repetitive script failures? ğŸ”âŒğŸ“œ
Inconsistent builds across systems? â‰ âš™ï¸ğŸ–¥ï¸

So how do we solve this decades of problem we face in every stage of SDLC, CICD, Deployments etc?
Is there any actual practical stable reliable ready solution available to tackle literally all these changes faced in IT?
Even if there is one, how many toolchain does it take to solve all these?
What would the world look like if we can actually reliably say "It works on my machine"?ğŸ˜³
Instead of accepting the overhead of maintaining the cracks and trading off cracks with isolation etc, is there any such omnipotent like solution to actually solve all these?

The ultimate solution to all these is
Nix. The one dependency solution.
Behold the power of Nix and how it changes the world in the history of IT.
